---
layout: post
title : MachineLearing学习笔记(10)--SVD简化数据
description : MachineLearing学习笔记(10)--SVD简化数据
category : 研究
tags : MachineLearning study-note Python
keywords : 
---

## SVD 奇异值分解
我们希望的是通过保存小部分信息，能够保存大部分数据信息。

矩阵分解也有很多途经，常见的一种便是SVD，奇异值分解。对于N*M的矩阵，可以分解为U，Sigma，VT三个矩阵的乘积。N*M=(N*N) * (N*M) * (M*M)的分解形式。其中Sigma是个对角矩阵， 对角线上的值为矩阵的奇异值。在之前的PCA之中，我们同样是对协方差矩阵求特征值，保留前K大的特征值，即数据中的重要特征。那么奇异值也是如此，奇异值便是Data * Data^T的特征值的平方根。

## 数据压缩
我们保留前K大的奇异值，构造新的sigma矩阵，N*M=(N*K)*(K*K)*(K*M)，那么信息量将大大减少。

那么K如何确定呢，我们一般默认，包含数据原有大部分信息，或者90%的信息，将奇异值平方，求和后，求前k大的平方和能够达到90%的能量信息就好了！或者默认的，取前30%，10维取3，1k维取0.3k之类的。

可以实验一下，将data复原一下，观察一下误差。

## 数据降维
SVD分解之后，除了得到奇异值之外，还有两个矩阵U和VT。这两个都是对数据进行映射的变换矩阵，U矩阵会将列映射到K个空间中。VT矩阵会将行映射到K个空间中去。

##推荐系统
推荐系统就是找相似性。首先两个向量的相似度统计有许多 方法 ，欧式距离 啊，皮尔逊相关，余弦相似度啊之类的。

比如说有n个用户对m样菜品进行打分，未尝过分数为0。那么推荐系统要为某个用户推荐的话，就需要在用户未尝过的当中对每样物品预测分数，然后得到分数最高或者前K高的。

预测的话，对于当前需要预测的物品，然后考虑其它物品，通过所有用户对这两个物品的评价的向量，统计之间的相关性，来预测这个物品。

## 代码君
[代码君前往cxlove's github](https://github.com/cxlove/MachineLearning/tree/master/SVD)