---
layout: post
title : MachineLearing学习笔记(1)--K-Means聚类算法
description : MachineLearing学习笔记(1)--K-Means聚类算法
category : 研究
keywords :
tags : MachineLearning study-note Python
---


　　**因为是临时实验需要，所以并没有在前面先进行ML方面的理论学习，直接入门了聚类算法，so下面有什么没有深入了解或者说错的地方，请指出。**

#KMeans

### 简述
　　首先KMeans是一种非常简单的聚类算法。

　　KMeans是典型的无监督学习，这一点很好理解。像其它算法，我有大概了解下，类别是已有固定的，然后将样本放入到已有的类别中。这属于监督学习，是有参考的。而KMeans更多的像自动学习。完全是将样本之间相似的归为一类。

### 步骤
　　k-means是要将样例聚类成k个簇：

1.  随机选取k个点为k个类的质心
2.  不断迭代以下过程：每个点选取与他距离最近的质心，归为那个类。然后对于每个类选取新的质心。直到聚类不再变化。

　　 在这个重复迭代的过程中，就会逐渐收敛。这个其实是很好理解的，在任何一个状态下，我们总能调整cluster，使得整个聚类的误差在减小。而这个值显然会趋于稳定饱和。而又正是因为如此，整个函数又不是凸函数，所以很可能走上**<del>歪道</del>**，陷于局部最优，这也是KMeans的一个很严重的缺点。

### 缺点
1.  可能陷入局部最优 。
2.  大规模样本数据而言，收敛很慢，相对而言，时间空间要求都挺高。

#二分K-均值算法
　　这个是为了克服局部最小值的问题。

　　也很好理解，就是开始是一个类，然后每次选一个类，一分为二。选的话，就是使得SSE尽可能小的那个类分割。所谓SSE就是这个类中所有点到质心的距离和。

**<del>吐槽：k-means真有用吗，效果好差的说</del>**

　　需要多跑几次，然后选取较小的SSE作为最终的方案。

#改进
　　在KMeans或者biKMeans之后，可以作一些改进，进行一些cluster上的调整。

　　选取SSE最大的类，再次聚类成两个类，即一分为二。然后将所有类中的两个进行合并。以此迭代。

　　至于合并可以有两种方案：(1)选择质心最低的两个(2)选择合并之后SSE最小的。

　　<del>cxlove觉得这样更能改进局部最小的情况。</del>

#结果君
贴一张结果图，分成4个类。

![result](/images/ML1_1.png)

#代码君
[前往cxlove's github寻找代码君](https://github.com/cxlove/MachineLearning/tree/master/K-Means)