---
layout: post
title : 过拟合与正则化的那些事
description : 过拟合与正则化的那些事
category : 研究
tags : study-note MachineLearning
keywords : 
---

<font color="red">**注：本文纯属总结，后附参考链接。<del>（如有侵权，联系本人。）</del> --cxlove**</font>
  

## 过拟合(Overfitting)

    
过拟合是ML中最为重要的问题之一，也无须多说。

### What

通俗的说，就是在训练样本中误差极小，但是测试样本上的误差非常大。如下图所示，最右虽然完美的拟合了所有的样本点，但是显然所得模型不是我们所想要的。

![多项式拟合中的过拟合](/images/Regular_1.png)

### Why

至于过拟合产生的原因，也是极其复杂的。

1.  最通俗，也是最难解释的当属模型复杂度。下图表现了模型复杂度和误差之间的关系，当模型过于复杂时，就会出现过拟合。如上面的多项式拟合；

![误差与模型复杂度的关系](/images/Regular_2.png)

2.  样本太少，特征维度过高。简而言之，便是样本数量与特征的关系不足以充分训练出模型，那自然效果很差；
3.  数据中噪音干扰过大。
4.  训练迭代过多，拟合了很多噪音以及没有代表性的数据，比如说决策树过于复杂、精细。

<font color="red">**<del>PS：Why和How有时候是两码事。并不是防止过拟合就不能使用复杂的模型，也不是样本越多，特征越少就好。</del>**</font>


### How
****

方法呢也有很多。

1.  权值衰减；
2.  交叉验证；
3.  特征工程；
4.  噪音处理；
5.  正则化。

### 题外话: bias & variance

>引用一个例子，一次打靶实验，目标是为了打到10环，但是实际上只打到了7环，那么这里面的Error就是3。但是瞄准的是9环。

偏差(bias)：bias就是说模型预期与真实值之间的差值。在上面的例子中，模型预期是9环，实际目标是10环，那么bias为1。

方差(variance)：variance反映的是模型预期与实际预测之间的差值。字面理解也知道是由于不稳定性带来的误差。上例中，虽然瞄准的是9环，但是由于射击的不稳定性，只打了7环，那么由于variance带来的误差便是2。

![bias与variance](/images/Regular_3.png)

实际情况中，我们并不知道真实模型，于是我们引入了可能的模型，自会引入bias，又由于我们的样本数据是有限且有噪音的，所以又添加了variance。

上图也很好的反应了过拟合的问题。

**<del>好了，接下来才是正题，本来就是主要讲正则化的嘛。</del>**

## 正则化(Regularization)

监督学习的目标便是规则化参数的同时最小化误差，最小化误差当然是保证我们的数据尽可能的拟合样本数据，而规则化参数是为了防止过拟合。因为参数过多，过于复杂，会导致模型复杂度的上升，虽然误差会很小，但是容易过拟合，这和我们之前提到过拟合的重要原因符合。

### What
大多数的监督学习，都可以抽象成以下这个优化问题：$$\min\limits_w\ \ loss(y - f(x, w) ) + \lambda * \Omega (w)$$

### How

使用正则化表达式来规则化参数，也有多种形式，主要介绍常见的L0,L1,L2范数。

#### L0范数

L0范数指参数中非0的个数，简单粗暴，如下式。
$$\Omega (w) = \sum !!w_i$$

我们之前说希望模型尽可能简单，那么参数最少，当然就越简单了，这太直白露骨了。所以L0范数尽可能地实现稀疏化，易于理解。

#### L1范数

L1范数指各个参数的绝对值之和，如下式：
$$\Omega (w) = \sum |w_i|$$

L1范数更多的也是实现稀疏化，我们会在下一章节详细介绍，于是问题来了，L0这么直白，那为何又要有L1的存在呢。

##### L1相对于L0的意义

1.  L0不易求解，你想吧，一个二值函数，完全没法解最优化；
2.  L1却是L0的最佳凸近似，绝对值函数见得过了，相对而言好处理多了。

#### L2范数

L2范数指各个参数的平方和，如下式：
$$\Omega (w) = \sum w_i ^ 2$$

L2范数更多的是可以防止过拟合，具体也在下一章节介绍。

### Why

这一章节主要介绍以上3种范数为什么能在规则化参数的同时实现稀疏化以及防止过拟合。

#### 直观感觉

我们随便拿个过拟合的例子来看，通常情况像这种过拟合的情况，拟合的曲线波动很多，函数值变化幅度大，那当自变量变化的同时，一起保持着因变量如此大幅度的波动，通常可以猜想系数的绝对值很大。而从另外一个角度看，函数的导数值波动也很大，只有系数绝对值足够大，才能引入大波动的导数值。那么从这个角度来看，L1,L2范数都能很好的控制系数的大小。

![过拟合示例](/images/Regular_4.png)

#### Occam’s razor 之L1

>Occam’s razor：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单的模型才是最好的模型，也就是应该选择的模型。

我们将介绍L1范数是如何调整模型实现稀疏化的。下面这张图选自PRML，用于线性回归模型，只有两个参数，在边是L2范数，右边是L1范数。蓝色圆圈是优化过程中误差形成的等高线，而红色的是正则化项，即参数规则化的受限条件。L2范数是平方和，所以是个圆，L1范数是绝对值的和，所以是个菱形。

试想，我们优化的目标便是使得误差尽可能的小，而且要满足参数规则化的受限条件，即最小的蓝色的圆和红线的交界处。从两幅图中就能非常直观的看出，对于右图，蓝色圆圈慢慢扩大，很大概率是先和菱形的角相交，而这意味着这什么，交点在坐标轴上，也就是有部分参数为0，如图中w1=0，既然参数出现了权值为0，这不就更好的实现了稀疏化吗，模型变得更加简单了。这是完全符合奥卡姆剃刀原理的。

![PRML图例](/images/Regular_5.png)

##### 稀疏化的好处

1.  符合剃刀原理，这也许是很重要的。
2.  特征选择，如果参数为0，那么所对应的特征就没有意义，某种程序上实现了自动的特征选择；
3.  易于解释，能够很好的知道高维特征中哪些维是对结果产生影响的。

#### 高斯先验之L2

下面这张图源于其它人分享的课件。可以看出我们添加的L2正则化项，可以理解成给模型添加了一个零均值协方差为$\frac{1}{\alpha}$的高斯分布先验。$\alpha$越大，意味着协方差越小，模型的variance越小，越稳定。而$\alpha=0$的话，即不添加正则化项，意味着模型的协方差为正无穷，即这个先验的约束非常小。

![高斯分布先验](/images/Regular_6.png)

理解这个问题之前，还需要一些先验知识，从上面的式子可以看出，我们相当于添加了一个$\alpha ^ {-1} * I$，即除对角线为0，这意味着什么，不同特征之间的相关性为0。这又是我们所添加的先验知识，如果特征之间存在相关性，那么我们总能先处理掉相关性，再进行模型拟合，所以默认我们认为现有的特征之间不具有相关性。

##### 题外话：ill-conditioned

优化问题的一大难点之一，便是ill-conditioned。这个用来衡量一个模型对于输入的敏感度，当输入改变一点点，输出却变化很大，说明模型对于输入很敏感，这是不好的现象。那么这种情况就称为ill-conditioned。

而引入L2范数，即之前提到的，添加一个$\alpha^1*I$项可以用助于处理codition number不好的情况，具体推导请看zouxy09的blog，见reference。

## 参考(Reference)

知乎问题：<a href="http://www.zhihu.com/question/20700829" target="_blank">机器学习中使用「正则化来防止过拟合」到底是一个什么原理？为什么正则化项就可以防止过拟合？</a>

知乎问题：<a href="http://www.zhihu.com/question/27068705" target="_blank">机器学习中的Bias(偏差)，Error(误差)，和Variance(方差)有什么区别和联系？</a>

zouxy09's blog：<a href="http://blog.csdn.net/zouxy09/article/details/24971995" target="_blank">机器学习中的范数规则化之（一）L0、L1与L2范数</a>

波波语录：<a href="http://www.qqtu8.com/f/20061208211555.gif" target="_blank">波波语录精髓</a>

